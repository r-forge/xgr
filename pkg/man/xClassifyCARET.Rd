% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xClassifyCARET.r
\name{xClassifyCARET}
\alias{xClassifyCARET}
\title{Function to combine predictor matrix in a supervised manner via classifying algorithms.}
\usage{
xClassifyCARET(df_predictor, GSP, GSN, algorithm = c("glm", "bayesglm",
"glmnet", "lda", "rda", "pls", "svmRadial", "knn", "nnet", "gbm",
"xgbLinear", "xgbTree", "LogitBoost", "cforest", "rf", "myrf"),
nfold = 3, nrepeat = 10, seed = 825,
cv.aggregateBy = c("logistic", "Ztransform", "fishers",
"orderStatistic", "none"), verbose = TRUE)
}
\arguments{
\item{df_predictor}{a data frame with columns as predictors, with their
predictive scores inside it. This data frame must has row names}

\item{GSP}{a vector containing Gold Standard Positive (GSP)}

\item{GSN}{a vector containing Gold Standard Negative (GSN)}

\item{algorithm}{classifying algorithm. It can be one of "gbm" for
Gradient Boosting Machine (GBM), "svmRadial" for Support Vector
Machines with Radial Basis Function Kernel (SVM), "rda" for Regularized
Discriminant Analysis (RDA), "knn" for k-nearest neighbor (KNN), "pls"
for Partial Least Squares (PLS), "nnet" for Neural Network (NNET), "rf"
for Random Forest (RF), "myrf" for customised Random Forest (RF),
"cforest" for Conditional Inference Random Forest, "glmnet" for glmnet,
"glm" for Generalized Linear Model (GLM), "bayesglm" for Bayesian
Generalized Linear Model (BGLM), "LogitBoost" for Boosted Logistic
Regression (BLR), "xgbLinear" for eXtreme Gradient Boosting as linear
booster (XGBL), "xgbTree" for eXtreme Gradient Boosting as tree booster
(XGBT)}

\item{nfold}{an integer specifying the number of folds for cross
validataion. Per fold creates balanced splits of the data preserving
the overall distribution for each class (GSP and GSN), therefore
generating balanced cross-vallidation train sets and testing sets. By
default, it is 3 meaning 3-fold cross validation}

\item{nrepeat}{an integer specifying the number of repeats for cross
validataion. By default, it is 10 indicating the cross-validation
repeated 10 times}

\item{seed}{an integer specifying the seed}

\item{cv.aggregateBy}{the aggregate method used to aggregate results
from k-fold cross validataion. It can be either "orderStatistic" for
the method based on the order statistics of p-values, or "fishers" for
Fisher's method, "Ztransform" for Z-transform method, "logistic" for
the logistic method. Without loss of generality, the Z-transform method
does well in problems where evidence against the combined null is
spread widely (equal footings) or when the total evidence is weak;
Fisher's method does best in problems where the evidence is
concentrated in a relatively small fraction of the individual tests or
when the evidence is at least moderately strong; the logistic method
provides a compromise between these two. Notably, the aggregate methods
'Ztransform' and 'logistic' are preferred here}

\item{verbose}{logical to indicate whether the messages will be
displayed in the screen. By default, it sets to TRUE for display}
}
\value{
an object of class "sClass", a list with following components:
\itemize{
\item{\code{prediction}: a data frame of nrow X 5 containing priority
information, where nrow is the number of rows in the input data frame,
and the 5 columns are "GS" (either 'GSP', or 'GSN', or 'NEW'), "name"
(row names of the input data frame), "rank" (ranks of the priority
scores), "pvalue" (the cross-fold aggregated p-value of being GSP,
per-fold p-value converted from empirical cumulative distribution of
the probability of being GSP), "priority" (sqrt(-log10(pvalue))
rescaled into [0,100]])}
\item{\code{predictor}: a data frame, which is the same as the input
data frame but inserting two additional columns ('GS' and 'name')}
\item{\code{performance}: a data frame of 1+nPredictor X 4 containing
the supervised/predictor performance info, where nPredictor is the
number of predictors, and the 4 columns are "auroc" (AUC values),
"fmax" (F-max values), "amx" (maximum accuracy), and "direction" ('+'
indicating the higher score the better prediction; '-' indicating the
higher score the worse prediction)}
\item{\code{importance}: a data frame of nPredictor X 2 containing the
predictor importance info, where nPredictor is the number of
predictors, two columns are predictor importance measures
("MeanDecreaseAccuracy" and "MeanDecreaseGini") .
"MeanDecreaseAccuracy" sees how worse the model performs without each
predictor (a high decrease in accuracy would be expected for very
informative predictors), while "MeanDecreaseGini" measures how pure the
nodes are at the end of the tree (a high score means the predictor was
important if each predictor is taken out)}
\item{\code{parameter}: NULL or a data frame detailing tuning
parameters and their associated AUC, Sensitivity and Specificity.}
\item{\code{model}: the best model.}
\item{\code{algorithm}: the classifying algorithm.}
\item{\code{cv_model}: a list of models, results from per-fold train
set}
\item{\code{cv_prob}: a data frame of nrow X 2+nfold containing the
probability of being GSP, where nrow is the number of rows in the input
data frame, nfold is the number of folds for cross validataion, and the
first two columns are "GS" (either 'GSP', or 'GSN', or 'NEW'), "name"
(gene names), and the rest columns storing the per-fold probability of
being GSP}
\item{\code{cv_auroc}: a data frame of 1+nPredictor X 4+nfold
containing the supervised/predictor ROC info (AUC values), where
nPredictor is the number of predictors, nfold is the number of folds
for cross validataion, and the first 4 columns are "median" (the median
of the AUC values across folds), "mad" (the median of absolute
deviation of the AUC values across folds), "min" (the minimum of the
AUC values across folds), "max" (the maximum of the AUC values across
folds), and the rest columns storing the per-fold AUC values}
\item{\code{cv_fmax}: a data frame of 1+nPredictor X 4+nfold containing
the supervised/predictor PR info (F-max values), where nPredictor is
the number of predictors, nfold is the number of folds for cross
validataion, and the first 4 columns are "median" (the median of the
F-max values across folds), "mad" (the median of absolute deviation of
the F-max values across folds), "min" (the minimum of the F-max values
across folds), "max" (the maximum of the F-max values across folds),
and the rest columns storing the per-fold F-max values}
\item{\code{call}: the call that produced this result}
}
}
\description{
\code{xClassifyCARET} is supposed to integrate predictor matrix in a
supervised manner via classifying algorithm. It requires three inputs:
1) Gold Standard Positive (GSP) targets; 2) Gold Standard Negative
(GSN) targets; 3) a predictor matrix containing genes in rows and
predictors in columns, with their predictive scores inside it. It
returns an object of class 'sClass'.
}
\note{
It will depend on whether a package "caret" and its suggested packages
have been installed. It can be installed via:
\code{BiocManager::install(c("caret","e1071","gbm","kernlab","klaR","pls","nnet","randomForest","party","glmnet","arm","caTools","xgboost"))}.
}
\examples{
\dontrun{
# Load the library
library(XGR)
}
RData.location <- "http://galahad.well.ox.ac.uk/bigdata"
\dontrun{
sClass <- xClassifyCARET(df_predictor, GSP, GSN)
}
}
